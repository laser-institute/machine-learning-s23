---
title: "Machine Learning Learning Lab 3: Feature Engineering"
subtitle: "Overview Presentation"
author: "**Dr. Joshua Rosenberg**"
institute: "LASER Institute"
date: '`r format(Sys.time(), "%B %d, %Y")`'
output:
  xaringan::moon_reader:
    css:
     - default
     - css/laser.css
     - css/laser-fonts.css
    lib_dir: libs                        # creates directory for libraries
    seal: false                          # false: custom title slide
    nature:
      highlightStyle: default         # highlighting syntax for code
      highlightLines: true               # true: enables code line highlighting 
      highlightLanguage: ["r"]           # languages to highlight
      countIncrementalSlides: false      # false: disables counting of incremental slides
      ratio: "16:9"                      # 4:3 for standard size,16:9
      slideNumberFormat: |
       <div class="progress-bar-container">
        <div class="progress-bar" style="width: calc(%current% / %total% * 100%);">
        </div>
       </div>
---

class: clear, title-slide, inverse, center, top, middle

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r, echo=FALSE}
# then load all the relevant packages
pacman::p_load(pacman, knitr, tidyverse, readxl)
```

```{r xaringan-panelset, echo=FALSE}
xaringanExtra::use_panelset()
```

```{r xaringanExtra-clipboard, echo=FALSE}
# these allow any code snippets to be copied to the clipboard so they 
# can be pasted easily
htmltools::tagList(
  xaringanExtra::use_clipboard(
    button_text = "<i class=\"fa fa-clipboard\"></i>",
    success_text = "<i class=\"fa fa-check\" style=\"color: #90BE6D\"></i>",
  ),
  rmarkdown::html_dependency_font_awesome()
)
```

```{r xaringan-extras, echo=FALSE}
xaringanExtra::use_tile_view()

```

## \# `r rmarkdown::metadata$title`

### `r rmarkdown::metadata$author`

### `r format(Sys.time(), "%B %d, %Y")`

------------------------------------------------------------------------

# Purpose and Agenda

Having fit and interpreted our machine learning model, how do we make
our model better? That's the focus of this learning lab. There are three
core ideas, pertaining to feature engineering, resampling, and the
random forest algorithm. We again use the OULAD data.

## What we'll do in this presentation

-   Discussion 1
-   Key Concept: Feature Engineering (Part B)
-   Key Concept: Resampling and cross-validation
-   Key Concept: The Random Forest algorithm
-   Code-along
-   Discussion 2
-   Introduction to the other parts of this learning lab

------------------------------------------------------------------------

# Discussion 1

.panelset\[

.panel\[.panel-name\[Background\]

-   Having discussed ways of interpreting how good a predictive model
    is, we can consider how to make our model better, having a rigorous
    framework for answering that question.

\]

.panel\[.panel-name\[Conceptual Overview\]

-   How good is a good enough model *for which metrics*?

\]

.panel\[.panel-name[Coding Walkthrough]

-   For your research context, how predictively good does your model
    need to be to be used with actual people (i.e., teachers,
    administrators, parents, and students)?

\] \]

------------------------------------------------------------------------

# Key Concept: Feature Engineering (Part B)

.panelset\[

.panel\[.panel-name\[How?\]

-   Well, what if we just add the values for these variables directly
-   But, that ignores that they are at different time points
    -   We could include the time point variable, but that is (about)
        the same for every student
-   OULAD interaction data is this way: the number of *clicks* per day
-   This data is sometimes called *log-trace* or *clickstream* data

*What are some other ideas?* \]

.panel\[.panel-name\[How?\]

**A few (other) options**

-   Raw data points
-   Their mean
-   Their maximum
-   Their variability (standard deviation)
-   Their linear slope
-   Their quadratic slope

**Each of these may derive from a single *variable* but may offer
predictive utility as distinct *features***

\]

.panel\[.panel-name\[How?\]

Here's a time stamp:

```{r, echo = FALSE}
Sys.time()
```

**How could this variable be used as a predictor variable?**

\]

.panel\[.panel-name\[How?\]

-   Removing those with "near-zero variance"
-   Removing ID variables and others that *should not be* informative
-   Imputing missing values
-   Extract particular elements (i.e., particular *days* or *times*)
    from time-related data
-   Categorical variables: Dummy coding, combining categories
-   Numeric variables: Normalizing ("standardizing")

**In this lab, we focus on using [OULAD
data](https://analyse.kmi.open.ac.uk/open_dataset) on interactions**

\] \]

------------------------------------------------------------------------

# Key Concept: Resampling and cross-validation

.panelset\[

.panel\[.panel-name\[Data\]

# The importance of training data

-   Training data is what we use to "learn" from data
-   A "check" on your work is your predictions on *test* set data
    -   *Train data*: Outcome data that you use to fit your model
    -   *Validation data*<sup>1</sup>: Data you use to select a
        particular algorithm (not often used, as we'll discuss!)
    -   *Test ("hold-out") data*: Data that you do not use in any way to
        train your model

\]

.panel\[.panel-name\[Data\]

In LL1, we fit and interpreted a single *fit of our model* (80%
training, 20% testing). What if we decided to *add new features* or
*change existing features*?

We'd need to use the same training data to tune a new model---and the
same testing data to evaluate its performance. **But**, this could lead
to fitting a model based on how well we predict the data that happened
to end up in the test set.

We could be optimizing our model for our testing data; when used with
new data, our model could make poor predictions.

\]

.panel\[.panel-name\[Data\]

-   In short, a challenges arises when we wish to use our training data
    *more than once*

-   Namely, if we repeatedly training an algorithm on the same data and
    then make changes, we may be tailoring our model to specific
    features of the testing data

-   This is a *very common and pervasive problem* in machine learning
    applications

-   Resampling conserves our testing data; we don't have to spend it
    until we've finalized our model

\]

.panel\[.panel-name\[Data\]

-   Resampling involves blurring the boundaries between training and
    testing data, *but only for the training split of the data*

-   Specifically, it involves combining these two portions of our data
    into one, iteratively considering some of the data to be for
    "training" and some for "testing"

-   Then, fit measures are **averaged** across these different samples

\]

.panel\[.panel-name\[KFCV\]

## *k*-folds cross validation (KFCV)

-   One of the most common forms of resampling is *k*-folds cross
    validation
    -   Here, some of the data is considered to be a part of the
        *training* set
    -   The remaining data is a part of the *testing* set
-   How many sets (samples taken through resampling)?
    -   This is determined by *k*, number of times the data is resampled
    -   When *k* is equivalent to the number of rows in the data, i.e.
        "Leave One Out Cross-Validation" (LOOCV)

\]

.panel\[.panel-name\[KFCV\]

```{r, include = FALSE}
d <- tibble(id = 1:100, var_a = runif(100), var_b = runif(100))
```

```{r}
d %>% head(2)
```

Using *k* = 10, how can we split *n* = 100 cases into ten distinct
training and testing sets?

*First resampling*

```{r, echo = TRUE}
train <- d[1:90, ]
test <- d[91:100, ]
# then, train the model (using train) and calculate a fit measure (using test)
# repeat for train: 1:80, 91:100, test: 81:90, etc.
# ... through the tenth resampling, after which the fit measures are averaged
```

\]

.panel\[.panel-name\[Determining k\]

# But how do you determine what *k* should be?

-   A *historically common value* for *k* has been 10
-   But, as computers have grown in processing power, setting *k* equal
    to the number of rows in the data has become more common

\] \]

------------------------------------------------------------------------

# Random Forests

.panelset\[

.panel\[.panel-name\[Background\]

-   *Random forests* are extensions of classification trees
-   *Classification trees* are a type of algorithm that use conditional
    logic ("if-then" statements) in a *nested* manner
    -   For instance, here's a *very, very* simple tree (from
        [APM](https://link.springer.com/book/10.1007/978-1-4614-6849-3)):

\]

.panel\[.panel-name\[Algorithm\]

```{code, echo = TRUE}
if Predictor B >= 0.197 then
| if Predictor A >= 0.13 then Class = 1
| else Class = 2
else Class = 2
```

-   Measures are used to determine the splits in such a way that
    classifies observations into small, homogeneous groups (using
    measures such as the Gini index and entropy measure)

\]

.panel\[.panel-name\[More complexity\]

```{code, echo = TRUE}
if Predictor B >= 0.197 then
| if Predictor A >= 0.13 then
    | if Predictor C < -1.04 then Class = 1
    | else Class = 2
else Class = 3
```

As you can imagine, with many variables, these trees can become very
complex

\]

.panel\[.panel-name\[Key points\]

-   A Random forest is an extension of decision tree modeling whereby a
    collection of decision trees are sequentially **estimated using
    training data** - and **validated/tested using testing data**
-   Different *samples* of predictors are sampled for inclusion in each
    individual tree
-   Highly complex and non-linear relationships between variables can be
    estimated
-   Each tree is independent of every other tree\
-   For classification ML, the final output is the category/group/class
    selected by individual trees
-   For regression ML, the mean or average prediction of the individual
    trees is returned \]

.panel\[.panel-name\[And more...\]

-   There are several important tuning parameters for these models:
    -   the number of predictor variables that are randomly sampled for
        each split (`mtry`)
    -   the minimum number of data points required to execute a split
        into branches (`min_n`)
    -   the number of trees estimated as a part of the "forest"
        (`trees`)
-   These tuning parameters, broadly, balance predictive performance
    with the training data with how well the model will perform on new
    data

\] \]

------------------------------------------------------------------------

# Coding Walkthrough

.panelset\[

.panel\[.panel-name\[ggplot2\]

```{r, eval = FALSE, echo = TRUE}
library(ggrepel)
library(tidyverse)

ggplot(starwars, aes(x = height, label = name)) +
    geom_histogram()
```

\]

.panel\[.panel-name\[ggplot2\]

```{r, eval = FALSE, echo = TRUE}
library(ggrepel)

ggplot(starwars, aes(x = height, y = mass, label = name)) +
    geom_point() +
    geom_label_repel()
```

\]

.panel\[.panel-name\[vfcv\]

```{r, eval = FALSE, echo = TRUE}
library(tidymodels)

# during Step 1
kfcv <- vfold_cv(data_train, v = 20) # this differentiates this from what we did before
# before, we simple used data_train to fit our model
kfcv
```

\]

.panel\[.panel-name\[vfcv\]

```{r, eval=FALSE, echo = TRUE}
# during step 3
my_mod <- rand_forest() %>% # different
    set_engine("ranger", importance = "impurity") %>% # different and with importance
    set_mode("classification")
```

\]

.panel\[.panel-name\[vfcv\]

```{r, eval = FALSE, echo = TRUE}
fitted_model_resamples <- fit_resamples(my_wf, 
                                        resamples = vfcv, # different
                                        metrics = class_metrics) # instead of fit()
```

\]

.panel\[.panel-name\[vfcv\]

```{r, eval = FALSE, echo = TRUE}
collect_metrics(fitted_model_resamples)
```

\]

.panel\[.panel-name\[vip\]

```{r, eval = FALSE, echo = TRUE}
# during Step 5
final_fit %>% 
    pluck(".workflow", 1) %>%   
    pull_workflow_fit() %>% 
    vip(num_features = 10)
```

\]

\]

------------------------------------------------------------------------

# Discussion 2

.panelset\[

.panel\[.panel-name\[Reflecting\]

-   Why is resampling (cross-validation) important?

\]

.panel\[.panel-name\[Applying\]

-   Which feature engineering steps might you need to take with the data
    (or kind of data) you plan to use?

\] \]

------------------------------------------------------------------------

# Introduction to the other parts of this learning lab

.panelset\[

.panel\[.panel-name\[Readings\]

Baker, R. S., Esbenshade, L., Vitale, J., & Karumbaiah, S. (2023). Using
Demographic Data as Predictor Variables: a Questionable Choice. *Journal
of Educational Data Mining, 15*(2), 22-52.

And Rodriguez et al. (2021) and Gobert et al. (2013) (*optional*) \]

.panel\[.panel-name\[Case Study\]

-   Using (filtered) interaction data (whole data
    [here](https://analyse.kmi.open.ac.uk/open_dataset))
-   Adding features related to *activity type*
-   Once again interpreting the change in our predictions (a bit more on
    the bias-variance trade-off
    [here](https://jrosen48.github.io/ML-in-Science-Education-Workshop-Materials/#23))
    (through slide 30)

\]

.panel\[.panel-name\[Badge\]

-   Considering the use of demographic data as features
-   Adding *activity-specific* interaction types

\] \]

------------------------------------------------------------------------

# *fin*

We hope to see you in the ML topic area learning labs!

-   *Dr. Joshua Rosenberg* (jmrosenberg\@utk.edu;
    <https://joshuamrosenberg.com)>
-   *Dr. Peng He* (hepeng1\@msu.edu)

[These slides (Introductory
Presentation)](https://laser-institute.github.io/machine-learning/introductory-presentation.html#1)

[General troubleshooting tips for R and
RStudio](https://docs.google.com/document/d/14Jc-KG3m5k1BvyKWqw7KmDD21IugU5nV5edfJkZyspY/edit)
